# -*- coding: utf-8 -*-
"""Pruning Methods.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Jz-Zcg_DOrcvZW_qtvBdEKLHMUmY5CT4
"""

!pip install tensorflow-model-optimization

import tensorflow as tf
from tensorflow.keras import layers, models, datasets
import numpy as np
import matplotlib.pyplot as plt

# Load and preprocess CIFAR-10 dataset
(x_train, y_train), (x_test, y_test) = datasets.cifar10.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0
y_train, y_test = tf.keras.utils.to_categorical(y_train, 10), tf.keras.utils.to_categorical(y_test, 10)

# Define a simple CNN model
def create_model():
    model = models.Sequential([
        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),
        layers.MaxPooling2D((2, 2)),
        layers.Conv2D(64, (3, 3), activation='relu'),
        layers.MaxPooling2D((2, 2)),
        layers.Conv2D(64, (3, 3), activation='relu'),
        layers.Flatten(),
        layers.Dense(64, activation='relu'),
        layers.Dense(10, activation='softmax')
    ])
    return model

# Compile and train the model
base_model = create_model()
base_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
base_model.fit(x_train, y_train, epochs=5, validation_split=0.2)
base_model.evaluate(x_test, y_test, verbose=2)

# Fine-grained pruning (manual implementation)
def fine_grained_pruning(model, sparsity_ratio):
    pruned_model = tf.keras.models.clone_model(model)
    for layer in pruned_model.layers:
        if isinstance(layer, layers.Dense) or isinstance(layer, layers.Conv2D):
            weights, biases = layer.get_weights()
            threshold = np.percentile(np.abs(weights), sparsity_ratio * 100)

            # Ensure at least one weight remains
            if np.all(np.abs(weights) < threshold):
                threshold = np.min(np.abs(weights))  # Retain the smallest non-zero weight

            weights[np.abs(weights) < threshold] = 0
            layer.set_weights([weights, biases])
    return pruned_model

pruned_model_fg = fine_grained_pruning(base_model, sparsity_ratio=0.5)
pruned_model_fg.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Fine-tune the pruned model
pruned_model_fg.fit(x_train, y_train, epochs=5, validation_split=0.2)
pruned_model_fg.evaluate(x_test, y_test, verbose=2)

# Vector-level pruning (manual implementation using sparsity constraints)
# Vector-level pruning (manual implementation using sparsity constraints)
def vector_pruning(model, threshold=0.1):
    pruned_model = tf.keras.models.clone_model(model)

    for i, layer in enumerate(pruned_model.layers):
        if isinstance(layer, layers.Conv2D):
            # Get weights and biases
            weights, biases = layer.get_weights()

            # Calculate L2 norm for each filter (vector)
            # L2 norm is computed over the entire filter (height, width, input_channels)
            filter_l2_norms = np.sqrt(np.sum(weights**2, axis=(0, 1, 2)))  # Shape: (num_filters,)

            # Create a mask to retain filters with L2 norm > threshold
            mask = filter_l2_norms > threshold

            # Ensure at least one filter is retained
            if not np.any(mask):
                mask[np.argmax(filter_l2_norms)] = True  # Keep the filter with the highest L2 norm

            # Prune the filters
            pruned_weights = weights[..., mask]
            pruned_biases = biases[mask] if biases is not None else None

            # Update the layer configuration by creating a new Conv2D layer
            # with the pruned weights and biases
            new_layer = layers.Conv2D(
                filters=pruned_weights.shape[-1],  # Updated number of filters
                kernel_size=layer.kernel_size,
                strides=layer.strides,
                padding=layer.padding,
                activation=layer.activation,
                kernel_initializer=tf.keras.initializers.Constant(pruned_weights),
                bias_initializer=tf.keras.initializers.Constant(pruned_biases)
            )

            # Replace the original layer with the new layer
            pruned_model.layers[i] = new_layer

    # Recompile the pruned model
    pruned_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

    return pruned_model

# Apply vector-level pruning to the base model
vector_pruned_model = vector_pruning(base_model, threshold=0.1)
vector_pruned_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Fine-tune the vector-pruned model
vector_pruned_model.fit(x_train, y_train, epochs=5, validation_split=0.2)
vector_pruned_model.evaluate(x_test, y_test, verbose=2)

# Plot accuracy vs. pruning ratio for comparison
pruning_ratios = [0.1, 0.2, 0.5, 0.7]
fine_grained_accuracies = []
vector_pruned_accuracies = []

for ratio in pruning_ratios:
    # Fine-grained pruning
    pruned_model = fine_grained_pruning(base_model, sparsity_ratio=ratio)
    pruned_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    pruned_model.fit(x_train, y_train, epochs=3, validation_split=0.2, verbose=0)
    acc = pruned_model.evaluate(x_test, y_test, verbose=0)[1]
    fine_grained_accuracies.append(acc)

    # Vector-level pruning
    pruned_model = vector_pruning(base_model, threshold=ratio)
    pruned_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    pruned_model.fit(x_train, y_train, epochs=3, validation_split=0.2, verbose=0)
    acc = pruned_model.evaluate(x_test, y_test, verbose=0)[1]
    vector_pruned_accuracies.append(acc)

# Plot results
plt.plot(pruning_ratios, fine_grained_accuracies, label='Fine-Grained Pruning')
plt.plot(pruning_ratios, vector_pruned_accuracies, label='Vector-Level Pruning')
plt.xlabel('Pruning Ratio')
plt.ylabel('Accuracy')
plt.title('Pruning Comparison')
plt.legend()
plt.show()

import tensorflow as tf
from tensorflow.keras import layers, models, datasets
import numpy as np
import matplotlib.pyplot as plt
import time

# Load and preprocess CIFAR-10 dataset
(x_train, y_train), (x_test, y_test) = datasets.cifar10.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0
y_train, y_test = tf.keras.utils.to_categorical(y_train, 10), tf.keras.utils.to_categorical(y_test, 10)

# Define a simple CNN model
def create_model():
    model = models.Sequential([
        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),
        layers.MaxPooling2D((2, 2)),
        layers.Conv2D(64, (3, 3), activation='relu'),
        layers.MaxPooling2D((2, 2)),
        layers.Conv2D(64, (3, 3), activation='relu'),
        layers.Flatten(),
        layers.Dense(64, activation='relu'),
        layers.Dense(10, activation='softmax')
    ])
    return model

# Compile and train the model
base_model = create_model()
base_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
base_model.fit(x_train, y_train, epochs=5, validation_split=0.2)
base_model.evaluate(x_test, y_test, verbose=2)

# Fine-grained pruning (manual implementation)
def fine_grained_pruning(model, sparsity_ratio):
    pruned_model = tf.keras.models.clone_model(model)
    for layer in pruned_model.layers:
        if isinstance(layer, layers.Dense) or isinstance(layer, layers.Conv2D):
            weights, biases = layer.get_weights()
            threshold = np.percentile(np.abs(weights), sparsity_ratio * 100)

            # Ensure at least one weight remains
            if np.all(np.abs(weights) < threshold):
                threshold = np.min(np.abs(weights))  # Retain the smallest non-zero weight

            weights[np.abs(weights) < threshold] = 0
            layer.set_weights([weights, biases])
    return pruned_model

pruned_model_fg = fine_grained_pruning(base_model, sparsity_ratio=0.5)
pruned_model_fg.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Fine-tune the pruned model
pruned_model_fg.fit(x_train, y_train, epochs=5, validation_split=0.2)
pruned_model_fg.evaluate(x_test, y_test, verbose=2)

# Vector-level pruning (manual implementation using sparsity constraints)
def vector_pruning(model, threshold=0.1):
    pruned_model = tf.keras.models.clone_model(model)

    for i, layer in enumerate(pruned_model.layers):
        if isinstance(layer, layers.Conv2D):
            # Get weights and biases
            weights, biases = layer.get_weights()

            # Calculate L2 norm for each filter (vector)
            # L2 norm is computed over the entire filter (height, width, input_channels)
            filter_l2_norms = np.sqrt(np.sum(weights**2, axis=(0, 1, 2)))  # Shape: (num_filters,)

            # Create a mask to retain filters with L2 norm > threshold
            mask = filter_l2_norms > threshold

            # Ensure at least one filter is retained
            if not np.any(mask):
                mask[np.argmax(filter_l2_norms)] = True  # Keep the filter with the highest L2 norm

            # Prune the filters
            pruned_weights = weights[..., mask]
            pruned_biases = biases[mask] if biases is not None else None

            # Update the layer configuration by creating a new Conv2D layer
            # with the pruned weights and biases
            # Check if any filters are retained after pruning
            if pruned_weights.shape[-1] > 0:  # Check if any filters remain
                new_layer = layers.Conv2D(
                    filters=pruned_weights.shape[-1],  # Updated number of filters
                    kernel_size=layer.kernel_size,
                    strides=layer.strides,
                    padding=layer.padding,
                    activation=layer.activation,
                    kernel_initializer=tf.keras.initializers.Constant(pruned_weights),
                    bias_initializer=tf.keras.initializers.Constant(pruned_biases)
                )

                # Replace the original layer with the new layer
                pruned_model.layers[i] = new_layer
            else:
                print(f"Skipping layer {i} due to zero filters after pruning.")
                # You can potentially add a default Conv2D layer here with minimal filters


    # Recompile the pruned model
    pruned_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

    return pruned_model

# Apply vector-level pruning to the base model
vector_pruned_model = vector_pruning(base_model, threshold=0.1)
vector_pruned_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Fine-tune the vector-pruned model
vector_pruned_model.fit(x_train, y_train, epochs=5, validation_split=0.2)
vector_pruned_model.evaluate(x_test, y_test, verbose=2)

# Define a function to calculate the model size
def get_model_size(model):
    # Calculate the total number of parameters (weights) in the model
    return np.sum([np.prod(w.shape) for w in model.get_weights()])

# Function to measure inference speed
def measure_inference_speed(model, x_test, batch_size=128):
    start_time = time.time()
    model.predict(x_test, batch_size=batch_size)
    end_time = time.time()
    return end_time - start_time

# Function to measure fine-tuning time
def measure_training_time(model, x_train, y_train, epochs=5):
    start_time = time.time()
    model.fit(x_train, y_train, epochs=epochs, validation_split=0.2, verbose=0)
    end_time = time.time()
    return end_time - start_time

# Compare fine-grained and channel-level pruning across various metrics

# Plot accuracy vs. pruning ratio for comparison
pruning_ratios = [0.1, 0.2, 0.5, 0.7]
fine_grained_accuracies = []
vector_pruned_accuracies = []
fine_grained_sizes = []
channel_pruned_sizes = []
fine_grained_inference_times = []
channel_pruned_inference_times = []
fine_grained_training_times = []
channel_pruned_training_times = []

for ratio in pruning_ratios:
    # Fine-grained pruning
    pruned_model = fine_grained_pruning(base_model, sparsity_ratio=ratio)
    pruned_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    pruned_model.fit(x_train, y_train, epochs=3, validation_split=0.2, verbose=0)
    acc = pruned_model.evaluate(x_test, y_test, verbose=0)[1]
    fine_grained_accuracies.append(acc)

    # Calculate and store metrics for fine-grained pruning
    fine_grained_sizes.append(get_model_size(pruned_model))
    fine_grained_inference_times.append(measure_inference_speed(pruned_model, x_test))
    fine_grained_training_times.append(measure_training_time(pruned_model, x_train, y_train, epochs=3))

    # Vector-level pruning
    pruned_model = vector_pruning(base_model, threshold=ratio)
    pruned_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    pruned_model.fit(x_train, y_train, epochs=3, validation_split=0.2, verbose=0)
    acc = pruned_model.evaluate(x_test, y_test, verbose=0)[1]
    vector_pruned_accuracies.append(acc)

    # Calculate and store metrics for vector-level pruning (using channel_pruned lists for now)
    channel_pruned_sizes.append(get_model_size(pruned_model))
    channel_pruned_inference_times.append(measure_inference_speed(pruned_model, x_test))
    channel_pruned_training_times.append(measure_training_time(pruned_model, x_train, y_train, epochs=3))

# Plot results
plt.plot(pruning_ratios, fine_grained_accuracies, label='Fine-Grained Pruning')
plt.plot(pruning_ratios, vector_pruned_accuracies, label='Vector-Level Pruning')
plt.xlabel('Pruning Ratio')
plt.ylabel('Accuracy')
plt.title('Pruning Comparison')
plt.legend()
plt.show()

# Plot the results

fig, axs = plt.subplots(2, 2, figsize=(14, 12))

# Plot Model Size (Compression)
axs[0, 0].plot(pruning_ratios, fine_grained_sizes, label='Fine-Grained Pruning')
axs[0, 0].plot(pruning_ratios, channel_pruned_sizes, label='Channel-Level Pruning')
axs[0, 0].set_xlabel('Pruning Ratio')
axs[0, 0].set_ylabel('Model Size (Number of Parameters)')
axs[0, 0].set_title('Model Size Comparison')
axs[0, 0].legend()

# Plot Inference Speed
axs[0, 1].plot(pruning_ratios, fine_grained_inference_times, label='Fine-Grained Pruning')
axs[0, 1].plot(pruning_ratios, channel_pruned_inference_times, label='Channel-Level Pruning')
axs[0, 1].set_xlabel('Pruning Ratio')
axs[0, 1].set_ylabel('Inference Time (Seconds)')
axs[0, 1].set_title('Inference Speed Comparison')
axs[0, 1].legend()

# Plot Training Time
axs[1, 0].plot(pruning_ratios, fine_grained_training_times, label='Fine-Grained Pruning')
axs[1, 0].plot(pruning_ratios, channel_pruned_training_times, label='Channel-Level Pruning')
axs[1, 0].set_xlabel('Pruning Ratio')
axs[1, 0].set_ylabel('Training Time (Seconds)')
axs[1, 0].set_title('Training Time Comparison')
axs[1, 0].legend()

plt.tight_layout()
plt.show()